---
- hosts: localhost
  connection: local

  vars_files:
    - test_vars.yml

  tasks:
    - block:

         ## Generating the testname for deployment
        - include_tasks: /utils/fcm/create_testname.yml

         ## RECORD START-OF-TEST IN LITMUS RESULT CR
        - include_tasks: "/utils/fcm/update_litmus_result_resource.yml"
          vars:
            status: 'SOT'

        - name: Getting the compute node name
          shell: kubectl get nodes --no-headers | grep -v master | awk {'print $1'}
          register: nodes

        - block:
            - name: set the value for the disk count to fetch the unclaimed blockDevice from each node
              set_fact:
                disk_count: "{{ item.value.count }}"
              loop: "{{ lookup('dict', bd_count) }}"
              when: "'{{ pool_type }}' in item.key"

            # Creating the blockdevice template from blockdevice_stripe.j2 jinja template for each node
            - name: Add node labels for each nodes and create blockdevice template
              template:
                src: ./blockdevice.j2
                dest: ./blockdevice-{{ item[0] }}.yml
              with_together:
                - "{{ nodes.stdout_lines }}"

            - name: Add the block devices for each node's block device template
              include_tasks: add_blockdevice.yml
              with_items: "{{ nodes.stdout_lines }}"
              loop_control:
                loop_var: outer_item

            # Insert the blockdevice template created for each nodes into cspc spec
            # blockinfile module will insert the external_files/block.
            # marker line template will be replaced with the values in marker_begin (default="BEGIN") and marker_end (default="END").
            - name: Include the blockdevice template in the CSPC spec
              blockinfile:
                dest: ./cspc.yml
                marker: "## {{ item }} Ansible Config ##"
                insertafter: pools
                state: present
                block: |
                  {{ lookup('file', './blockdevice-{{ item }}.yml') }}
              with_items:
                - "{{ nodes.stdout_lines }}"

            - set_fact:
                device_count: "{{ blockDevice|length}}"              

            - name: Replacing the pool name in CSPC spec
              replace:
                path: ./cspc.yml
                regexp: "pool-name"
                replace: "{{ pool_name }}"

            - name: Replacing the pool group id in CSPC spec
              replace:
                path: ./cspc.yml
                regexp: "pool-group"
                replace: "{{ pool_group }}"

            - name: Replacing the storage class name in CSPC spec
              replace:
                path: ./cspc.yml
                regexp: "sc_name"
                replace: "{{ sc_name }}"

            - name: Replacing the namespace in CSPC spec
              replace:
                path: ./cspc.yml
                regexp: "operator_ns"
                replace: "{{ operator_ns }}"

            - name: Replacing the pool type in CSPC spec
              replace:
                path: ./cspc.yml
                regexp: "pool-type"
                replace: "{{ pool_type }}"

            - name: Display cspc.yml for verification 
              debug: var=item
              with_file:
              - "cspc.yml"

            - name: Create cstor disk pool
              shell: kubectl apply -f cspc.yml
              args:
                executable: /bin/bash

            - name: Verify if cStor Disk Pool are Running
              shell: >
                kubectl get pods -n {{ operator_ns }} -l openebs.io/cstor-pool-cluster={{ pool_name }}
                --no-headers -o custom-columns=:status.phase
              args:
                executable: /bin/bash
              register: pool_count
              until: "((pool_count.stdout_lines|unique)|length) == 1 and 'Running' in pool_count.stdout"
              retries: 30
              delay: 10

            - name: Get cStor Disk Pool names to verify the container statuses
              shell: >
                kubectl get pods -n {{ operator_ns }} -l openebs.io/cstor-pool-cluster={{ pool_name }}
                --no-headers -o=custom-columns=NAME:".metadata.name"
              args:
                executable: /bin/bash
              register: cstor_pool_pod

            - name: Get the runningStatus of pool pod
              shell: >
                kubectl get pod {{ item }} -n {{ operator_ns }}
                -o=jsonpath='{range .status.containerStatuses[*]}{.state}{"\n"}{end}' |
                grep -w running | wc -l
              args:
                executable: /bin/bash
              register: runningStatusCount
              with_items: "{{ cstor_pool_pod.stdout_lines }}"
              until: "runningStatusCount.stdout == device_count"
              delay: 30
              retries: 10
          when: deploy_mode == "create"


        - block:

            - name: Remove the CStor Pool Cluster
              shell: >
                kubectl delete cspc {{ pool_name }} -n openebs
              args: 
                execuatble: /bin/bash

            - name: Verify if the cStor pool pods are deleted
              shell:
                kubectl get pods -n {{ operator_ns }} -l openebs.io/cstor-pool-cluster={{ pool_name }}
                --no-headers -o custom-columns=:status.phase
              args:
                executable: /bin/bash
              register: pool_status
              until: "'No resources found. in pool_status.stdout"
              retries: 30
              delay: 10

         when: deploy_mode =="delete"

        - set_fact:
            flag: "Pass"
    
      rescue:
          - set_fact:
              flag: "Fail"
  
      always:
            ## RECORD END-OF-TEST IN LITMUS RESULT CR
          - include_tasks: /utils/fcm/update_litmus_result_resource.yml
            vars:
              status: 'EOT'