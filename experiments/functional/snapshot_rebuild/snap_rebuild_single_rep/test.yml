---
## Test involved.
#Check if the application is running already.
#Selct one of the replicas.
#Introduce packet drop in the selected replica.
#Create snapshot.
#check if the snapshot is not present in the degraded pod.
#Bring up the replica.
#Check if the snapshot is rebuilt.

- hosts: localhost
  connection: local

  vars_files:
    - test_vars.yml

  tasks:
    - block:

      ## Generating the testname.
        - include_tasks: /utils/fcm/create_testname.yml

         ## RECORD START-OF-TEST IN LITMUS RESULT CR
        - include_tasks: "/utils/fcm/update_litmus_result_resource.yml"
          vars:
            status: 'SOT'

          ## Obtain cstor replica pods
        - include_tasks: /funclib/kubectl/k8s_snapshot_clone/snap_rebuild_prerequisites.yml
          vars:
            app_ns: "{{ namespace }}"
            pvc_name: "{{ app_pvc }}"
            app_label: "{{ application_label }}"
            ns: "{{ operator_ns }}"

        - name: Display details
          debug:
            msg:
              - "cstor_replicas: {{ cstor_replicas_pod }}"

        # #Select any cstor replica pod to inject packet loss
        - set_fact:
            replica_pod: "{{ cstor_replicas_pod | list | first }}"

        ## Inducing the packet drop on the above pool pod.
        - include_tasks: "/chaoslib/openebs/inject_packet_loss_tc.yml"
          vars:
            status: "induce"
            target_pod: "{{ replica_pod }}"
            operator_namespace: "{{ operator_ns }}"

        ## If there are no IOs happening on the volume, replica won't get disconnected
        - name: Check if the targeted replica is disconnected.
          shell: >
            kubectl exec -it {{ cstor_target.stdout }} -n {{ operator_ns }} --container  cstor-istgt 
            -- istgtcontrol -q replica |json_pp | jq '.volumeStatus[0].replicaStatus[].Mode' | grep Healthy | wc -l
          register: connected_replica
          until: "(connected_replica.stdout|int) == ((healthy_pods.stdout|int) - 1)"
          delay: 15
          retries: 30
        
        - name: Creating snapshot.
          include_tasks: /funclib/kubectl/k8s_snapshot_clone/create_snapshot.yml 
          vars:
            app_ns: "{{ namespace }}"
            pvc_name: "{{ app_pvc }}"

        - debug:
            msg: "snapshotname: {{ snapshot_name }}"

        - name: Checking the snapshot status after removing packet loss
          include_tasks: /funclib/kubectl/k8s_snapshot_clone/snapshot_status.yml
          vars:
            app_ns: "{{ namespace }}"
            snap_name: "{{ snapshot_name }}"
            pvc_name: "{{ app_pvc }}"
            action: 'Success'

        - debug:
            msg: "snapid_name: {{ snapid_name }}"

        - name: Checking if the snapshot is not created in the degraded replica.
          shell: kubectl exec -ti {{ replica_pod }} -n {{ operator_ns }} --container cstor-pool -- zfs list -t snapshot
          args:
            executable: /bin/bash
          register: list_snap
          failed_when: "snapid_name in list_snap.stdout"

        ## Removing the packet drop rule by including the relevant util..
        - include_tasks: "/chaoslib/openebs/inject_packet_loss_tc.yml"
          vars:
            status: "remove"
            target_pod: "{{ replica_pod }}"
            operator_namespace: "{{ operator_ns }}"

        - name: Check if the snapshot is being rebuilt.
          shell: kubectl exec -it {{ replica_pod }} -n {{ operator_ns }} --container cstor-pool -- zfs list -t snapshot
          args:
            executable: /bin/bash
          register: list_snap
          until: "snapid_name in list_snap.stdout"
          delay: 30
          retries: 20
 
        - name: Check if the snapshot rebuild is successful.
          shell: >
            kubectl exec -ti {{ replica_pod }} -n {{ operator_ns }} -c cstor-pool 
            -- zfs stats |json_pp| grep "rebuildStatus" |awk -F ':' '{print $2}' | tr -d '"' | tr -d ','
          args:
            executable: /bin/bash
          register: snap_result
          until: "'DONE' in snap_result.stdout"
          delay: 20
          retries: 100

        - set_fact:
            flag: "Pass"

      rescue:
        - set_fact:
            flag: "Fail"

      always:
            ## RECORD END-OF-TEST IN LITMUS RESULT CR
        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'EOT'
